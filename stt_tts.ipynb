{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1IWp30SbxsKWXTIAsjwFjlpf6wtoX4bjg",
      "authorship_tag": "ABX9TyNns031t2GxJMoNdCeq+CF1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohanK-17/Voice-Conversion-System/blob/main/stt_tts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Voice Recording and Transcription Notebook\n",
        "\n",
        "This notebook allows you to record audio directly from your microphone in Google Colab. You press a button to start recording, speak, and press it again to stop. After recording, you can immediately listen to the audio in the notebook.\n",
        "\n",
        "The recorded audio is captured in WebM format and automatically converted to WAV, making it easy to process in Python. The WAV file is read into a NumPy array, giving access to the audio samples and sampling rate.\n",
        "\n",
        "This setup can be extended to save recordings, generate unique timestamped filenames, or transcribe audio using models like Whisper. It provides a simple and interactive way to capture, preview, and process voice recordings all within Colab.\n"
      ],
      "metadata": {
        "id": "s4xvt7WdGwHZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install all required libraries in a single cell**"
      ],
      "metadata": {
        "id": "lRxR0L7F6Cep"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuUl0y3ki3C6"
      },
      "source": [
        "\n",
        "!pip install -q gradio soundfile torch torchaudio huggingface_hub ruaccent datasets transformers faster-whisper ffmpeg-python coqui-tts speechbrain accelerate\n",
        "\n",
        "!pip install -q gradio soundfile torch torchaudio huggingface_hub ruaccent\n",
        "!git clone https://huggingface.co/ESpeech/ESpeech-TTS-1_RL-V2\n",
        "!pip install -q datasets\n",
        "!pip install -q torch torchaudio transformers soundfile\n",
        "!pip install -q faster-whisper ffmpeg-python\n",
        "!pip install -q coqui-tts\n",
        "!pip install -q speechbrain\n",
        "!pip install -q speechbrain transformers accelerate datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clone the ESpeech-TTS repository**"
      ],
      "metadata": {
        "id": "FiYSQwKZ6Lqh"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5824e568"
      },
      "source": [
        "!git clone https://huggingface.co/ESpeech/ESpeech-TTS-1_RL-V2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📝 End-to-End Voice Recording, Transcription, and Speech Synthesis\n",
        "\n",
        "This Colab cell implements a complete workflow for **recording, transcribing, and synthesizing speech**:\n",
        "\n",
        "1. **Record Audio in Browser**  \n",
        "   - Uses JavaScript via `IPython.display.HTML` to capture microphone input.\n",
        "   - Saves the recorded audio as a WAV file in a `recordings/` folder.\n",
        "\n",
        "2. **Transcribe Audio**  \n",
        "   - Loads the recorded audio using the **Faster-Whisper** model (`medium.en`).\n",
        "   - Detects language and generates a text transcript.\n",
        "   - Prints each segment with timestamps and stores the full transcription in `transcribed_text`.\n",
        "\n",
        "3. **Text-to-Speech (SpeechSynthesis)**  \n",
        "   - Uses **SpeechT5** with the HifiGan vocoder to generate speech from the transcribed text.\n",
        "   - Requires `speaker_embeddings` and the SpeechT5 `model` to be preloaded.\n",
        "   - Saves the synthesized speech as a WAV file and plays it directly in the notebook.\n",
        "\n",
        "4. **Output**  \n",
        "   - Displays the recorded audio player.\n",
        "   - Displays the synthesized speech player (if embeddings and model are available).\n",
        "   - All recordings are timestamped for easy reference.\n",
        "\n",
        "> This workflow enables end-to-end **voice cloning / speech synthesis experiments**, combining live recording, transcription, and TTS in a single interactive cell.\n"
      ],
      "metadata": {
        "id": "bxkVlCx76oCk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "140f28b8"
      },
      "source": [
        "import os, io, ffmpeg, torch\n",
        "from datetime import datetime\n",
        "from base64 import b64decode\n",
        "from IPython.display import HTML, display, Audio\n",
        "from faster_whisper import WhisperModel\n",
        "from transformers import SpeechT5Processor, SpeechT5HifiGan\n",
        "import soundfile as sf\n",
        "\n",
        "os.makedirs(\"recordings\", exist_ok=True)\n",
        "\n",
        "AUDIO_HTML = \"\"\"\n",
        "<script>\n",
        "var my_div = document.createElement(\"DIV\");\n",
        "var my_btn = document.createElement(\"BUTTON\");\n",
        "var t = document.createTextNode(\"Press to start recording\");\n",
        "my_btn.appendChild(t);\n",
        "my_div.appendChild(my_btn);\n",
        "document.body.appendChild(my_div);\n",
        "\n",
        "var base64data = 0;\n",
        "var reader;\n",
        "var recorder, gumStream;\n",
        "var recordButton = my_btn;\n",
        "\n",
        "var handleSuccess = function(stream) {\n",
        "  gumStream = stream;\n",
        "  recorder = new MediaRecorder(stream);\n",
        "  recorder.ondataavailable = function(e) {\n",
        "    reader = new FileReader();\n",
        "    reader.readAsDataURL(e.data);\n",
        "    reader.onloadend = function() {\n",
        "      base64data = reader.result;\n",
        "    }\n",
        "  };\n",
        "  recorder.start();\n",
        "};\n",
        "\n",
        "recordButton.innerText = \"Recording... press to stop\";\n",
        "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
        "\n",
        "function toggleRecording() {\n",
        "  if (recorder && recorder.state == \"recording\") {\n",
        "      recorder.stop();\n",
        "      gumStream.getAudioTracks()[0].stop();\n",
        "      recordButton.innerText = \"Saving...\"\n",
        "  }\n",
        "}\n",
        "\n",
        "function sleep(ms) {\n",
        "  return new Promise(resolve => setTimeout(resolve, ms));\n",
        "}\n",
        "\n",
        "var data = new Promise(resolve=>{\n",
        "recordButton.onclick = ()=>{\n",
        "  toggleRecording()\n",
        "  sleep(2000).then(() => {\n",
        "    resolve(base64data.toString())\n",
        "  });\n",
        "}\n",
        "});\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "def get_audio():\n",
        "    display(HTML(AUDIO_HTML))\n",
        "    data = eval_js(\"data\")\n",
        "    binary = b64decode(data.split(',')[1])\n",
        "\n",
        "    process = (\n",
        "        ffmpeg\n",
        "        .input('pipe:0')\n",
        "        .output('pipe:1', format='wav')\n",
        "        .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True)\n",
        "    )\n",
        "    output, _ = process.communicate(input=binary)\n",
        "    return output\n",
        "\n",
        "print(\"Click button, record, then stop...\")\n",
        "wav_bytes = get_audio()\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%b%d_%H-%M-%S\")\n",
        "filename = f\"recordings/{timestamp}.wav\"\n",
        "\n",
        "with open(filename, \"wb\") as f:\n",
        "    f.write(wav_bytes)\n",
        "\n",
        "print(f\"Saved audio → {filename}\")\n",
        "display(Audio(filename))\n",
        "\n",
        "#Transcribe using Faster-Whisper\n",
        "model_size = \"medium.en\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "whisper_model = WhisperModel(model_size, device=device)\n",
        "\n",
        "print(\"Transcribing audio...\")\n",
        "segments, info = whisper_model.transcribe(filename)\n",
        "\n",
        "print(f\"Detected language: {info.language}\")\n",
        "transcript = \"\"\n",
        "for segment in segments:\n",
        "    print(f\"[{segment.start:.2f}s → {segment.end:.2f}s] {segment.text}\")\n",
        "    transcript += segment.text + \" \"\n",
        "\n",
        "transcribed_text = transcript.strip()\n",
        "print(\"\\nTranscription complete.\")\n",
        "print(f\"Stored transcript:\\n{transcribed_text}\")\n",
        "\n",
        "#Voice tt5\n",
        "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n",
        "processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
        "\n",
        "if 'speaker_embeddings' in locals() and 'model' in locals():\n",
        "    inputs = processor(text=transcribed_text, return_tensors=\"pt\")\n",
        "    speech = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings, vocoder=vocoder)\n",
        "    output_file = f\"recordings/synthesized_{timestamp}.wav\"\n",
        "    sf.write(output_file, speech.numpy(), samplerate=16000)\n",
        "    print(f\"✅ Synthesized speech saved to {output_file}\")\n",
        "    display(Audio(output_file, rate=16000))\n",
        "else:\n",
        "    print(\"Speaker embeddings or SpeechT5 model not found. Please load them before generating speech.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📊 Audio Waveform Visualization\n",
        "\n",
        "This cell visualizes the amplitude over time for audio files:\n",
        "\n",
        "1. **Recorded Audio**  \n",
        "   - Plots the waveform of the audio captured from the microphone.\n",
        "   - X-axis: Time in seconds  \n",
        "   - Y-axis: Amplitude\n",
        "\n",
        "2. **Synthesized Audio**  \n",
        "   - If SpeechT5 synthesized speech was generated, its waveform is also plotted for comparison.\n",
        "   - Helps visually compare the recorded input and synthesized output.\n",
        "\n",
        "> This is useful for inspecting audio quality, duration, and amplitude patterns.\n"
      ],
      "metadata": {
        "id": "rpAPC-M864bg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "007f8411"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_waveform(audio_file_path, title=\"Audio Waveform\"):\n",
        "    import soundfile as sf\n",
        "    try:\n",
        "        audio, samplerate = sf.read(audio_file_path)\n",
        "        time = np.linspace(0., len(audio) / samplerate, len(audio))\n",
        "        plt.figure(figsize=(12, 4))\n",
        "        plt.plot(time, audio)\n",
        "        plt.xlabel(\"Time [s]\")\n",
        "        plt.ylabel(\"Amplitude\")\n",
        "        plt.title(f\"{title}: {audio_file_path}\")\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "    except Exception as e:\n",
        "        print(f\"Error plotting waveform: {e}\")\n",
        "\n",
        "plot_waveform(filename, title=\"Recorded Audio\")\n",
        "\n",
        "if 'output_file' in locals():\n",
        "    plot_waveform(output_file, title=\"Synthesized Audio\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📊 ASR and TTS Evaluation Metrics Visualization\n",
        "\n",
        "This cell demonstrates **hypothetical performance metrics** for ASR (Automatic Speech Recognition) and TTS (Text-to-Speech) models, along with **voice cloning evaluation**:\n",
        "\n",
        "1. **ASR Metrics (Word Error Rate - WER)**  \n",
        "   - Shows hypothetical WER scores for different ASR models.  \n",
        "   - Lower WER indicates better transcription accuracy.  \n",
        "   - Visualized using a bar chart.\n",
        "\n",
        "2. **TTS Subjective Metrics (Mean Opinion Score - MOS)**  \n",
        "   - Shows hypothetical MOS evaluations for TTS models across multiple dimensions:  \n",
        "     - Overall MOS  \n",
        "     - Fluency MOS  \n",
        "     - Prosody MOS  \n",
        "     - Quality MOS  \n",
        "   - Higher MOS indicates better perceived naturalness and quality.  \n",
        "   - Visualized with grouped bar charts to compare multiple metrics.\n",
        "\n",
        "3. **TTS Objective Metric (Voice Cloning: Cosine Similarity)**  \n",
        "   - Hypothetical cosine similarity between original and synthesized speaker embeddings.  \n",
        "   - Higher similarity (closer to 1) indicates better voice cloning fidelity.  \n",
        "   - Visualized with a simple bar chart.\n",
        "\n",
        "> These visualizations help compare **different ASR/TTS models** and evaluate **voice cloning quality**, both subjectively (MOS) and objectively (embedding similarity).  \n"
      ],
      "metadata": {
        "id": "TpJ8_v417EI7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ef7499bd"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "# Hypothetical data for demonstration\n",
        "# Replace with your actual evaluation results\n",
        "\n",
        "# ASR Metrics\n",
        "asr_data = {\n",
        "    'Model': ['Faster-Whisper (small)', 'Another ASR Model', 'Yet Another ASR Model'],\n",
        "    'WER (%)': [5.5, 8.2, 7.1] # Hypothetical Word Error Rates (lower is better)\n",
        "}\n",
        "df_asr = pd.DataFrame(asr_data)\n",
        "\n",
        "# TTS Subjective Metrics (from MOS evaluation)\n",
        "tts_subjective_data = {\n",
        "    'Model': ['SpeechT5', 'TTS Model 2', 'TTS Model 3'],\n",
        "    'Overall MOS': [4.2, 3.8, 4.5], # Hypothetical Mean Opinion Scores (higher is better)\n",
        "    'Fluency MOS': [4.3, 3.9, 4.4],\n",
        "    'Prosody MOS': [4.0, 3.5, 4.6],\n",
        "    'Quality MOS': [4.1, 3.7, 4.5]\n",
        "}\n",
        "df_tts_subjective = pd.DataFrame(tts_subjective_data)\n",
        "\n",
        "# TTS Objective Metric (for Voice Cloning)\n",
        "tts_objective_data = {\n",
        "    'Model': ['SpeechT5 (Cloned)', 'Another Cloning Model'],\n",
        "    'Cosine Similarity': [0.92, 0.88] # Hypothetical Cosine Similarity (closer to 1 is better)\n",
        "}\n",
        "df_tts_objective = pd.DataFrame(tts_objective_data)\n",
        "\n",
        "# --- Display DataFrames ---\n",
        "print(\"Hypothetical ASR Model Performance (WER):\")\n",
        "display(df_asr)\n",
        "\n",
        "print(\"\\nHypothetical TTS Model Subjective Performance (MOS):\")\n",
        "display(df_tts_subjective)\n",
        "\n",
        "print(\"\\nHypothetical Voice Cloning Performance (Cosine Similarity):\")\n",
        "display(df_tts_objective)\n",
        "\n",
        "\n",
        "# --- Visualization of ASR Word Error Rate ---\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(df_asr['Model'], df_asr['WER (%)'], color='skyblue')\n",
        "plt.ylabel('Word Error Rate (%)')\n",
        "plt.title('Hypothetical ASR Model Performance (WER)')\n",
        "plt.ylim(0, 10) # Adjust y-limit based on expected scores\n",
        "plt.show()\n",
        "\n",
        "# --- Visualization of TTS Subjective Metrics (MOS) ---\n",
        "bar_width = 0.2\n",
        "r1 = np.arange(len(df_tts_subjective['Model']))\n",
        "r2 = [x + bar_width for x in r1]\n",
        "r3 = [x + bar_width for x in r2]\n",
        "r4 = [x + bar_width for x in r3]\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(r1, df_tts_subjective['Overall MOS'], color='lightcoral', width=bar_width, edgecolor='grey', label='Overall MOS')\n",
        "plt.bar(r2, df_tts_subjective['Fluency MOS'], color='salmon', width=bar_width, edgecolor='grey', label='Fluency MOS')\n",
        "plt.bar(r3, df_tts_subjective['Prosody MOS'], color='tomato', width=bar_width, edgecolor='grey', label='Prosody MOS')\n",
        "plt.bar(r4, df_tts_subjective['Quality MOS'], color='orangered', width=bar_width, edgecolor='grey', label='Quality MOS')\n",
        "\n",
        "plt.ylabel('Mean Opinion Score (MOS)')\n",
        "plt.title('Hypothetical TTS Model Subjective Performance (MOS)')\n",
        "plt.ylim(0, 5.5) # MOS is typically on a scale of 1-5\n",
        "plt.xticks([r + bar_width*1.5 for r in range(len(df_tts_subjective['Model']))], df_tts_subjective['Model'])\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# --- Visualization of TTS Objective Metric (Cosine Similarity) ---\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(df_tts_objective['Model'], df_tts_objective['Cosine Similarity'], color='mediumseagreen')\n",
        "plt.ylabel('Cosine Similarity')\n",
        "plt.title('Hypothetical Voice Cloning Performance (Cosine Similarity)')\n",
        "plt.ylim(0, 1.1) # Cosine similarity is between -1 and 1, often 0 to 1 for embeddings\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ef2428b"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "\n",
        "audio_file_path = \"speecht5_hifigan_synthesized_speech.wav\"\n",
        "\n",
        "try:\n",
        "    audio, samplerate = sf.read(audio_file_path)\n",
        "\n",
        "\n",
        "    time = np.linspace(0., len(audio) / samplerate, len(audio))\n",
        "\n",
        "    # Plot the waveform\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.plot(time, audio)\n",
        "    plt.xlabel(\"Time [s]\")\n",
        "    plt.ylabel(\"Amplitude\")\n",
        "    plt.title(\"Synthesized Audio Waveform\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Audio file not found at {audio_file_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🗣️ Voice Cloning with SpeechT5 and Speaker Embedding\n",
        "\n",
        "This cell continues the workflow from previous steps and performs **voice cloning** using:\n",
        "\n",
        "1. **Speaker embedding extraction**:\n",
        "   - Loads a short reference audio (`ref_wav`) of the user's voice.\n",
        "   - Processes the audio to match the sample rate expected by the pre-trained SpeechBrain speaker recognition model (`spkrec-xvect-voxceleb`).\n",
        "   - Generates a speaker embedding tensor that captures the unique characteristics of the speaker's voice.\n",
        "\n",
        "2. **Text-to-Speech synthesis**:\n",
        "   - Uses the previously transcribed text (`transcribed_text`) from the recorded audio.\n",
        "   - Loads the pre-trained **SpeechT5** model, processor, and HifiGan vocoder.\n",
        "   - Generates a speech waveform in the **user's voice** by combining the text and speaker embedding.\n",
        "   - Saves the cloned speech to `cloned_speech.wav` and plays it inline.\n",
        "\n",
        "**Notes:**\n",
        "- The speaker embedding must have a batch dimension; if missing, it’s automatically added.\n",
        "- The generated speech is at 16 kHz, suitable for playback or further processing.\n"
      ],
      "metadata": {
        "id": "rZEreR5d74kr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0a2d23e"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "print(\"Please upload a short audio file of your voice.\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "\n",
        "if len(uploaded) == 0:\n",
        "  print(\"No file was uploaded.\")\n",
        "  ref_wav = None\n",
        "else:\n",
        "  ref_wav = list(uploaded.keys())[0]\n",
        "  print(f\"✅ Uploaded audio file: {ref_wav}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a7905c4"
      },
      "source": [
        "from speechbrain.pretrained import EncoderClassifier\n",
        "import torch\n",
        "\n",
        "speaker_model = EncoderClassifier.from_hparams(source=\"speechbrain/spkrec-xvect-voxceleb\", run_opts={\"device\":\"cuda\" if torch.cuda.is_available() else \"cpu\"})\n",
        "\n",
        "print(\"Speaker embedding model loaded successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54fcaab6"
      },
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from IPython.display import Audio\n",
        "\n",
        "speaker_embedding = None\n",
        "\n",
        "if ref_wav is not None:\n",
        "    print(f\"Loading audio from {ref_wav}...\")\n",
        "    try:\n",
        "        # Load audio (compatible with torchaudio returning tuple or SimpleNamespace)\n",
        "        audio_loaded = torchaudio.load(ref_wav)\n",
        "        if isinstance(audio_loaded, tuple):\n",
        "            audio_tensor, sample_rate = audio_loaded\n",
        "        else:  # SimpleNamespace in newer torchaudio\n",
        "            audio_tensor = audio_loaded.signal\n",
        "            sample_rate = audio_loaded.sample_rate\n",
        "\n",
        "        # Ensure mono\n",
        "        if audio_tensor.shape[0] > 1:\n",
        "            audio_tensor = audio_tensor.mean(dim=0, keepdim=True)\n",
        "\n",
        "        # Resample if needed\n",
        "        target_sr = getattr(speaker_model.hparams, \"sample_rate\", 16000)  # fallback to 16kHz\n",
        "        if sample_rate != target_sr:\n",
        "            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=target_sr)\n",
        "            audio_tensor = resampler(audio_tensor)\n",
        "            print(f\"Resampled audio to {target_sr} Hz\")\n",
        "        else:\n",
        "            print(f\"Audio sample rate matches model: {sample_rate} Hz\")\n",
        "\n",
        "        # Generate speaker embedding\n",
        "        print(\"Generating speaker embedding...\")\n",
        "        with torch.no_grad():\n",
        "            embeddings = speaker_model.encode_batch(audio_tensor)\n",
        "\n",
        "        speaker_embedding = embeddings.squeeze(0)\n",
        "        print(\"✅ Speaker embedding generated.\")\n",
        "        print(f\"Embedding shape: {speaker_embedding.shape}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing audio file: {e}\")\n",
        "        speaker_embedding = None\n",
        "else:\n",
        "    print(\"No audio file provided. Cannot generate speaker embedding.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjwcRuJwjE7C"
      },
      "source": [
        "\n",
        "print(speaker_model.hparams)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5ef356c"
      },
      "source": [
        "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\n",
        "import torch\n",
        "import soundfile as sf\n",
        "from IPython.display import display, Audio\n",
        "\n",
        "processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
        "model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\")\n",
        "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n",
        "\n",
        "if 'transcribed_text' in locals() and 'speaker_embedding' in locals() and speaker_embedding is not None:\n",
        "    print(\"Using transcribed text:\", transcribed_text)\n",
        "    print(\"Using speaker embedding with shape:\", speaker_embedding.shape)\n",
        "\n",
        "    inputs = processor(text=transcribed_text, return_tensors=\"pt\")\n",
        "\n",
        "    if speaker_embedding.ndim == 1:\n",
        "        speaker_embedding = speaker_embedding.unsqueeze(0)\n",
        "        print(\"Added batch dimension to speaker embedding. New shape:\", speaker_embedding.shape)\n",
        "\n",
        "    speech = model.generate_speech(inputs[\"input_ids\"], speaker_embedding, vocoder=vocoder)\n",
        "\n",
        "    output_file = \"cloned_speech.wav\"\n",
        "    sf.write(output_file, speech.numpy(), samplerate=16000)\n",
        "\n",
        "    print(\"Speech saved to:\", output_file)\n",
        "    display(Audio(output_file, rate=16000))\n",
        "else:\n",
        "    print(\"Transcribed text or speaker embedding not available. Cannot synthesize speech.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85d527e7"
      },
      "source": [
        "## Play cloned voice output\n",
        "\n",
        "### Subtask:\n",
        "Listen to the synthesized speech to evaluate the voice cloning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "084e47d6"
      },
      "source": [
        "**Reasoning**:\n",
        "Play the synthesized audio file named `cloned_speech.wav` to allow for subjective evaluation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4265e057"
      },
      "source": [
        "import IPython.display as ipd\n",
        "import os\n",
        "\n",
        "output_file = \"cloned_speech.wav\"\n",
        "\n",
        "if os.path.exists(output_file):\n",
        "    print(f\"Playing synthesized speech from {output_file}\")\n",
        "    display(ipd.Audio(output_file, rate=16000))\n",
        "else:\n",
        "    print(f\"Error: Synthesized speech file not found at {output_file}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-pqwZkeC1-Ag"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}